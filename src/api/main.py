"""
API Gateway para Sistema de Pron√≥stico Automatizado de Demanda Energ√©tica - EPM
=================================================================================

Endpoints:
    POST /predict - Genera predicci√≥n de 30 d√≠as con granularidad horaria
    GET /health - Estado del sistema
    GET /models - Informaci√≥n de modelos disponibles

Autor: Sistema EPM
Fecha: Noviembre 2024
Versi√≥n: 1.0.0
"""

from fastapi import FastAPI, HTTPException, status
from pydantic import BaseModel, Field, field_validator
from typing import Optional, List, Dict, Any, Tuple
from pathlib import Path
from datetime import datetime
import logging
import traceback
import pandas as pd
from fastapi.concurrency import run_in_threadpool
import os
from dotenv import load_dotenv
from openai import OpenAI
# Importar componentes del sistema
from src.pipeline.orchestrator import run_automated_pipeline
from src.models.trainer import ModelTrainer
from src.prediction.forecaster import ForecastPipeline
from src.prediction.hourly import HourlyDisaggregationEngine
from src.pipeline.update_csv import full_update_csv
from fastapi.concurrency import run_in_threadpool

# Cargar variables de entorno
load_dotenv()
# Configuraci√≥n de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Inicializar FastAPI
app = FastAPI(
    title="EPM Energy Demand Forecasting API",
    description="API Gateway para pron√≥stico automatizado de demanda energ√©tica",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# ============================================================================
# SCHEMAS DE REQUEST/RESPONSE
# ============================================================================

class PredictRequest(BaseModel):
    """Schema para solicitud de predicci√≥n"""
    # power_data_path: str = Field(
    #     ...,
    #     description="Ruta al archivo CSV con datos hist√≥ricos de demanda hasta el d√≠a anterior"
    # )
    # weather_data_path: Optional[str] = Field(
    #     'data/raw/clima_new.csv',
    #     description="Ruta al archivo CSV con datos meteorol√≥gicos API EPM (se usa por defecto data/raw/clima_new.csv si no se especifica)"
    # )
    # start_date: Optional[str] = Field(
    #     None,
    #     description="Fecha inicial para filtrar datos hist√≥ricos (formato: YYYY-MM-DD)"
    # )
    ucp: str = Field(
        None,
        description="Selecci√≥n de UCP para calculos"
    )
    end_date: Optional[str] = Field(
        None,
        description="Fecha final de datos hist√≥ricos (formato: YYYY-MM-DD)"
    )
    n_days: int = Field(
        30,
        description="N√∫mero de d√≠as a predecir",
        ge=1,
        le=90
    )
    force_retrain: bool = Field(
        False,
        description="Forzar reentrenamiento del modelo aunque exista uno. Si es True, entrena los 3 modelos y selecciona autom√°ticamente el mejor basado en rMAPE"
    )

    @field_validator( 'end_date')
    @classmethod
    def validate_date_format(cls, v: Optional[str]) -> Optional[str]:
        """Valida formato de fechas"""
        if v is not None:
            try:
                datetime.strptime(v, '%Y-%m-%d')
            except ValueError:
                raise ValueError('Formato de fecha inv√°lido. Usar YYYY-MM-DD')
        return v

    class Config:
        schema_extra = {
            "example": {
                "power_data_path": "data/raw/demanda_historica.csv",
                "weather_data_path": "data/raw/clima_historico.csv",
                "start_date": "2023-01-01",
                "end_date": "2024-11-27",
                "n_days": 30,
                "force_retrain": False
            }
        }


class HourlyPrediction(BaseModel):
    """Schema para predicci√≥n horaria de un d√≠a"""
    fecha: str = Field(..., description="Fecha de la predicci√≥n (YYYY-MM-DD)")
    dia_semana: str = Field(..., description="D√≠a de la semana")
    demanda_total: float = Field(..., description="Demanda total del d√≠a (MWh)")
    is_festivo: bool = Field(..., description="Si es d√≠a festivo")
    is_weekend: bool = Field(..., description="Si es fin de semana")
    metodo_desagregacion: str = Field(..., description="M√©todo usado (normal/special)")
    cluster_id: Optional[int] = Field(None, description="ID del cluster usado para desagregaci√≥n")
    P1: float = Field(..., description="Per√≠odo 1 (00:00-01:00) en MWh")
    P2: float = Field(..., description="Per√≠odo 2 (01:00-02:00) en MWh")
    P3: float = Field(..., description="Per√≠odo 3 (02:00-03:00) en MWh")
    P4: float = Field(..., description="Per√≠odo 4 (03:00-04:00) en MWh")
    P5: float = Field(..., description="Per√≠odo 5 (04:00-05:00) en MWh")
    P6: float = Field(..., description="Per√≠odo 6 (05:00-06:00) en MWh")
    P7: float = Field(..., description="Per√≠odo 7 (06:00-07:00) en MWh")
    P8: float = Field(..., description="Per√≠odo 8 (07:00-08:00) en MWh")
    P9: float = Field(..., description="Per√≠odo 9 (08:00-09:00) en MWh")
    P10: float = Field(..., description="Per√≠odo 10 (09:00-10:00) en MWh")
    P11: float = Field(..., description="Per√≠odo 11 (10:00-11:00) en MWh")
    P12: float = Field(..., description="Per√≠odo 12 (11:00-12:00) en MWh")
    P13: float = Field(..., description="Per√≠odo 13 (12:00-13:00) en MWh")
    P14: float = Field(..., description="Per√≠odo 14 (13:00-14:00) en MWh")
    P15: float = Field(..., description="Per√≠odo 15 (14:00-15:00) en MWh")
    P16: float = Field(..., description="Per√≠odo 16 (15:00-16:00) en MWh")
    P17: float = Field(..., description="Per√≠odo 17 (16:00-17:00) en MWh")
    P18: float = Field(..., description="Per√≠odo 18 (17:00-18:00) en MWh")
    P19: float = Field(..., description="Per√≠odo 19 (18:00-19:00) en MWh")
    P20: float = Field(..., description="Per√≠odo 20 (19:00-20:00) en MWh")
    P21: float = Field(..., description="Per√≠odo 21 (20:00-21:00) en MWh")
    P22: float = Field(..., description="Per√≠odo 22 (21:00-22:00) en MWh")
    P23: float = Field(..., description="Per√≠odo 23 (22:00-23:00) en MWh")
    P24: float = Field(..., description="Per√≠odo 24 (23:00-00:00) en MWh")
    senda_P1: Optional[float] = Field(None, description="Senda normalizada P1 (patr√≥n cluster 0-1)")
    senda_P2: Optional[float] = Field(None, description="Senda normalizada P2 (patr√≥n cluster 0-1)")
    senda_P3: Optional[float] = Field(None, description="Senda normalizada P3 (patr√≥n cluster 0-1)")
    senda_P4: Optional[float] = Field(None, description="Senda normalizada P4 (patr√≥n cluster 0-1)")
    senda_P5: Optional[float] = Field(None, description="Senda normalizada P5 (patr√≥n cluster 0-1)")
    senda_P6: Optional[float] = Field(None, description="Senda normalizada P6 (patr√≥n cluster 0-1)")
    senda_P7: Optional[float] = Field(None, description="Senda normalizada P7 (patr√≥n cluster 0-1)")
    senda_P8: Optional[float] = Field(None, description="Senda normalizada P8 (patr√≥n cluster 0-1)")
    senda_P9: Optional[float] = Field(None, description="Senda normalizada P9 (patr√≥n cluster 0-1)")
    senda_P10: Optional[float] = Field(None, description="Senda normalizada P10 (patr√≥n cluster 0-1)")
    senda_P11: Optional[float] = Field(None, description="Senda normalizada P11 (patr√≥n cluster 0-1)")
    senda_P12: Optional[float] = Field(None, description="Senda normalizada P12 (patr√≥n cluster 0-1)")
    senda_P13: Optional[float] = Field(None, description="Senda normalizada P13 (patr√≥n cluster 0-1)")
    senda_P14: Optional[float] = Field(None, description="Senda normalizada P14 (patr√≥n cluster 0-1)")
    senda_P15: Optional[float] = Field(None, description="Senda normalizada P15 (patr√≥n cluster 0-1)")
    senda_P16: Optional[float] = Field(None, description="Senda normalizada P16 (patr√≥n cluster 0-1)")
    senda_P17: Optional[float] = Field(None, description="Senda normalizada P17 (patr√≥n cluster 0-1)")
    senda_P18: Optional[float] = Field(None, description="Senda normalizada P18 (patr√≥n cluster 0-1)")
    senda_P19: Optional[float] = Field(None, description="Senda normalizada P19 (patr√≥n cluster 0-1)")
    senda_P20: Optional[float] = Field(None, description="Senda normalizada P20 (patr√≥n cluster 0-1)")
    senda_P21: Optional[float] = Field(None, description="Senda normalizada P21 (patr√≥n cluster 0-1)")
    senda_P22: Optional[float] = Field(None, description="Senda normalizada P22 (patr√≥n cluster 0-1)")
    senda_P23: Optional[float] = Field(None, description="Senda normalizada P23 (patr√≥n cluster 0-1)")
    senda_P24: Optional[float] = Field(None, description="Senda normalizada P24 (patr√≥n cluster 0-1)")

    class Config:
        schema_extra = {
            "example": {
                "fecha": "2024-12-01",
                "dia_semana": "Domingo",
                "demanda_total": 31500.0,
                "is_festivo": False,
                "is_weekend": True,
                "metodo_desagregacion": "normal",
                "P1": 1197.0, "P2": 1134.0, "P3": 1071.0,
                # ... (resto de per√≠odos)
            }
        }


class PredictResponse(BaseModel):
    """Schema para respuesta de predicci√≥n"""
    should_retrain: bool = Field(..., description="Indica si se recomienda reentrenar el modelo (true/false)")
    reason: str = Field(..., description="Raz√≥n por la cual se recomienda o no reentrenar")
    status: str = Field(..., description="Estado de la operaci√≥n")
    message: str = Field(..., description="Mensaje descriptivo")
    metadata: Dict[str, Any] = Field(..., description="Metadata de la predicci√≥n")
    predictions: List[HourlyPrediction] = Field(..., description="Array de predicciones diarias con desagregaci√≥n horaria")

    class Config:
        schema_extra = {
            "example": {
                "should_retrain": False,
                "reason": "Error dentro de l√≠mites aceptables (MAPE: 2.35%)",
                "status": "success",
                "message": "Predicci√≥n generada exitosamente para 30 d√≠as",
                "metadata": {
                    "fecha_generacion": "2024-11-28T10:30:00",
                    "modelo_usado": "xgboost_20241120_161937",
                    "dias_predichos": 30,
                    "fecha_inicio": "2024-11-28",
                    "fecha_fin": "2024-12-27",
                    "demanda_promedio": 31500.0,
                    "demanda_min": 28000.0,
                    "demanda_max": 35000.0,
                    "modelo_entrenado": False,
                    "metricas_modelo": {
                        "mape": 0.45,
                        "rmape": 3.2,
                        "r2": 0.946
                    }
                },
                "predictions": []  # Array de HourlyPrediction
            }
        }


class HealthResponse(BaseModel):
    """Schema para respuesta de health check"""
    status: str
    timestamp: str
    version: str
    components: Dict[str, Dict[str, Any]]


# ============================================================================
# FUNCIONES AUXILIARES
# ============================================================================

async def analyze_error_with_openai(
    ucp: str,
    error_type: str,
    mape_total: float,
    fecha_inicio: str,
    fecha_fin: str,
    dias_consecutivos: Optional[List[str]] = None
) -> str:
    """
    Analiza las posibles causas del error de predicci√≥n usando OpenAI con b√∫squeda en internet

    Args:
        ucp: Nombre del UCP (ej: 'Atlantico', 'Oriente')
        error_type: Tipo de error ('mensual', 'consecutivo', 'ambos')
        mape_total: MAPE mensual calculado
        fecha_inicio: Fecha inicio del periodo analizado
        fecha_fin: Fecha fin del periodo analizado
        dias_consecutivos: Lista de fechas con errores consecutivos > 5%

    Returns:
        str: An√°lisis detallado de OpenAI sobre posibles causas
    """
    try:
        # Obtener API key desde variables de entorno
        api_key = os.getenv('API_KEY')
        if not api_key:
            logger.warning("‚ö† API_KEY no encontrada en .env, saltando an√°lisis de OpenAI")
            return "An√°lisis no disponible (API_KEY no configurada)"

        # Inicializar cliente de OpenAI
        client = OpenAI(api_key=api_key)

        # Construir prompt seg√∫n tipo de error
        if error_type == 'consecutivo':
            dias_str = ', '.join(dias_consecutivos) if dias_consecutivos else '√∫ltimos 2 d√≠as'
            prompt = f"""Eres un analista energ√©tico experto. Necesito que investigues en internet las posibles causas de una anomal√≠a en la demanda energ√©tica.

**Contexto:**
- UCP: {ucp}, Colombia
- Fechas afectadas: {dias_str}
- Tipo de anomal√≠a: Dos d√≠as consecutivos con error de predicci√≥n superior al 5%
- Periodo analizado: {fecha_inicio} a {fecha_fin}

**Tarea:**
Busca en internet eventos, acontecimientos o situaciones que pudieron haber ocurrido en {ucp}, Colombia en las fechas {dias_str} que pudieron causar variaciones significativas en la demanda de energ√≠a el√©ctrica.

Considera:
- Eventos clim√°ticos extremos (tormentas, olas de calor/fr√≠o)
- Eventos p√∫blicos masivos (conciertos, partidos, festivales)
- D√≠as festivos locales o nacionales
- Apagones o fallas en el suministro
- Eventos pol√≠ticos o sociales
- Paros o manifestaciones
- cualquier otro acontecimiento relevante

Proporciona un an√°lisis conciso (m√°ximo 2-3 oraciones) con las causas m√°s probables encontradas."""

        elif error_type == 'mensual':
            # Extraer mes y a√±o de fecha_fin
            fecha_obj = datetime.strptime(fecha_fin, '%Y-%m-%d')
            mes_nombre = fecha_obj.strftime('%B %Y')

            prompt = f"""Eres un analista energ√©tico experto. Necesito que investigues en internet las posibles causas de una anomal√≠a en la demanda energ√©tica.

**Contexto:**
- UCP: {ucp}, Colombia
- Periodo: {mes_nombre} (del {fecha_inicio} al {fecha_fin})
- Tipo de anomal√≠a: Error mensual de predicci√≥n de {mape_total:.2f}% (superior al l√≠mite del 5%)

**Tarea:**
Busca en internet eventos, acontecimientos o condiciones que pudieron haber ocurrido en {ucp}, Colombia durante {mes_nombre} que pudieron causar variaciones significativas en la demanda de energ√≠a el√©ctrica durante todo el mes.

Considera:
- Condiciones clim√°ticas at√≠picas del mes (sequ√≠as, lluvias intensas, temperaturas anormales)
- Cambios en la actividad industrial o comercial
- Eventos recurrentes durante el mes
- Temporadas vacacionales o escolares
- Restricciones energ√©ticas o racionamientos
- Crecimiento poblacional o cambios demogr√°ficos
- cualquier otro acontecimiento relevante

Proporciona un an√°lisis conciso (m√°ximo 2-3 oraciones) con las causas m√°s probables encontradas."""

        else:  # 'ambos'
            dias_str = ', '.join(dias_consecutivos) if dias_consecutivos else '√∫ltimos 2 d√≠as'
            fecha_obj = datetime.strptime(fecha_fin, '%Y-%m-%d')
            mes_nombre = fecha_obj.strftime('%B %Y')

            prompt = f"""Eres un analista energ√©tico experto. Necesito que investigues en internet las posibles causas de una anomal√≠a severa en la demanda energ√©tica.

**Contexto:**
- UCP: {ucp}, Colombia
- Periodo mensual: {mes_nombre} (del {fecha_inicio} al {fecha_fin})
- Error mensual: {mape_total:.2f}% (superior al l√≠mite del 5%)
- D√≠as consecutivos afectados: {dias_str} (errores > 5%)

**Tarea:**
Busca en internet eventos o condiciones que pudieron causar tanto el error mensual sostenido como los picos espec√≠ficos en las fechas {dias_str} en {ucp}, Colombia.

Proporciona un an√°lisis conciso (m√°ximo 3-4 oraciones) con las causas m√°s probables encontradas, conectando los eventos puntuales con las tendencias mensuales."""

        logger.info(f"ü§ñ Consultando OpenAI (gpt-5-mini Responses API) para an√°lisis de causalidad ({error_type})...")

        # Llamar a OpenAI con b√∫squeda en internet habilitada
        # Nota: GPT-5-mini usa la nueva Responses API con web_search nativo
        response = await run_in_threadpool(
            lambda: client.responses.create(
                model="gpt-5-mini",  # Modelo GPT-5-mini con capacidad de b√∫squeda web
                input=[  # NOTA: Responses API usa 'input' en lugar de 'messages'
                    {
                        "role": "user",
                        "content": f"Eres un analista experto en sistemas energ√©ticos y demanda el√©ctrica en Colombia. Proporcionas an√°lisis concisos basados en informaci√≥n factual encontrada en internet.\n\n{prompt}"
                    }
                ],
                tools=[
                    {
                        "type": "web_search"  # Herramienta nativa de b√∫squeda web
                    }
                ]
            )
        )

        # La respuesta viene directamente en output_text en la nueva Responses API
        analysis = response.output_text.strip()
        logger.info(f"‚úì An√°lisis de OpenAI recibido: {len(analysis)} caracteres")

        return analysis

    except Exception as e:
        logger.error(f"Error en an√°lisis de OpenAI: {e}")
        logger.error(traceback.format_exc())
        return f"An√°lisis autom√°tico no disponible (error: {str(e)})"


def check_model_exists(ucp: str) -> Tuple[bool, Optional[Path]]:
    """
    Verifica si existe un modelo entrenado en el registro para un UCP espec√≠fico

    Args:
        ucp: Nombre del UCP (ej: 'Atlantico', 'Oriente')

    Returns:
        Tupla (existe: bool, path: Optional[Path])
    """
    models_dir = Path(f'models/{ucp}/trained')
    registry_path = Path(f'models/{ucp}/registry/champion_model.joblib')

    # Prioridad 1: Modelo campe√≥n en registry
    if registry_path.exists():
        logger.info(f"‚úì Modelo campe√≥n encontrado para {ucp}: {registry_path}")
        return True, registry_path

    # Prioridad 2: √öltimo modelo entrenado (por timestamp)
    if models_dir.exists():
        model_files = sorted(models_dir.glob('*.joblib'), key=lambda p: p.stat().st_mtime, reverse=True)
        if model_files:
            logger.info(f"‚úì √öltimo modelo entrenado encontrado para {ucp}: {model_files[0]}")
            return True, model_files[0]

    logger.warning(f"‚ö† No se encontr√≥ ning√∫n modelo entrenado para {ucp}")
    return False, None


def train_model_if_needed(df_with_features: pd.DataFrame,
                         ucp: str,
                         force_retrain: bool = False) -> Tuple[Path, Dict[str, Any]]:
    """
    Entrena los 3 modelos (XGBoost, LightGBM, RandomForest) y selecciona autom√°ticamente el mejor
    bas√°ndose en rMAPE de validaci√≥n

    Args:
        df_with_features: DataFrame con features procesados
        ucp: Nombre del UCP (ej: 'Atlantico', 'Oriente')
        force_retrain: Forzar reentrenamiento

    Returns:
        Tupla (model_path: Path, metrics: Dict con m√©tricas del mejor modelo)
    """
    model_exists, model_path = check_model_exists(ucp)

    if model_exists and not force_retrain:
        logger.info("‚úì Usando modelo existente (no se requiere entrenamiento)")
        # model_path ya est√° verificado que no es None
        assert model_path is not None
        return model_path, {}

    logger.info("="*80)
    logger.info("üîß INICIANDO ENTRENAMIENTO AUTOM√ÅTICO DE MODELOS (SIN LAGS)")
    logger.info("="*80)

    # IMPORTANTE: Excluir features de lag para evitar train-test mismatch en predicci√≥n recursiva
    FEATURES_LAG_TO_EXCLUDE = [
        'total_lag_1d', 'total_lag_7d', 'total_lag_14d',
        'p8_lag_1d', 'p8_lag_7d',
        'p12_lag_1d', 'p12_lag_7d',
        'p18_lag_1d', 'p18_lag_7d',
        'p20_lag_1d', 'p20_lag_7d',
        'total_day_change', 'total_day_change_pct'
    ]

    # Preparar datos para entrenamiento
    exclude_cols = ['FECHA', 'fecha', 'TOTAL', 'demanda_total'] + [f'P{i}' for i in range(1, 25)]
    exclude_cols.extend(FEATURES_LAG_TO_EXCLUDE)  # ‚Üê AGREGAR LAGS A EXCLUSI√ìN
    
    feature_cols = [col for col in df_with_features.columns if col not in exclude_cols]
    
    logger.info(f"  ‚ö†Ô∏è  Excluyendo {len(FEATURES_LAG_TO_EXCLUDE)} features de lag para mejor predicci√≥n recursiva")

    # Normalizar nombres de columnas
    target_col = 'TOTAL' if 'TOTAL' in df_with_features.columns else 'demanda_total'

    X = df_with_features[feature_cols].fillna(0)
    y = df_with_features[target_col].copy()

    # Eliminar NaN en target
    mask = ~y.isnull()
    X = X[mask]
    y = y[mask]

    logger.info(f"  Total registros: {len(X)}")
    logger.info(f"  Features totales: {len(df_with_features.columns) - len(exclude_cols)}")
    logger.info(f"  Features usados: {len(feature_cols)} (sin lags)")
    logger.info(f"  Features excluidos: {len(FEATURES_LAG_TO_EXCLUDE)} lags")

    # Split temporal (80% train, 20% validation)
    split_idx = int(len(X) * 0.8)
    X_train, X_val = X[:split_idx], X[split_idx:]
    y_train, y_val = y[:split_idx], y[split_idx:]

    logger.info(f"  Train: {len(X_train)} registros")
    logger.info(f"  Validation: {len(X_val)} registros")

    # Entrenar TODOS los modelos
    logger.info(f"\nüöÄ Entrenando los 3 modelos para {ucp} (XGBoost, LightGBM, RandomForest)...")

    trainer = ModelTrainer(
        optimize_hyperparams=False,  # Deshabilitado para velocidad
        cv_splits=3
    )

    trained_models = trainer.train_all_models(
        X_train=X_train,
        y_train=y_train,
        X_val=X_val,
        y_val=y_val,
        models=['xgboost', 'lightgbm', 'randomforest']
    )

    # Seleccionar autom√°ticamente el MEJOR modelo basado en rMAPE
    logger.info("\nüèÜ Seleccionando mejor modelo basado en rMAPE de validaci√≥n...")

    best_name, _, best_results = trainer.select_best_model(
        criterion='rmape',  # Usar rMAPE como criterio (m√©trica m√°s robusta)
        use_validation=True
    )

    logger.info(f"‚úì Mejor modelo seleccionado para {ucp}: {best_name.upper()}")
    logger.info(f"  MAPE: {best_results['val_metrics']['mape']:.4f}%")
    logger.info(f"  rMAPE: {best_results['val_metrics']['rmape']:.4f}")
    logger.info(f"  R¬≤: {best_results['val_metrics']['r2']:.4f}")
    logger.info(f"  MAE: {best_results['val_metrics']['mae']:.2f}")

    # Guardar TODOS los modelos en directorio espec√≠fico del UCP
    models_dir = Path(f'models/{ucp}/trained')
    models_dir.mkdir(parents=True, exist_ok=True)

    saved_paths = trainer.save_all_models(overwrite=True, output_dir=str(models_dir))

    # Path del mejor modelo
    best_model_path = saved_paths[best_name]
    # Asegurar que es un Path object
    best_model_path = Path(best_model_path) if isinstance(best_model_path, str) else best_model_path

    # Guardar el MEJOR modelo como campe√≥n en registry del UCP
    registry_dir = Path(f'models/{ucp}/registry')
    registry_dir.mkdir(parents=True, exist_ok=True)
    champion_path = registry_dir / 'champion_model.joblib'

    import shutil
    shutil.copy(best_model_path, champion_path)

    logger.info(f"\n‚úì Modelos guardados en: models/{ucp}/trained/")
    for name, path in saved_paths.items():
        status = "üèÜ CAMPE√ìN" if name == best_name else ""
        # Asegurar que path es un Path object
        path_obj = Path(path) if isinstance(path, str) else path
        logger.info(f"    {name}: {path_obj.name} {status}")

    logger.info(f"‚úì Modelo campe√≥n actualizado para {ucp}: {champion_path}")
    logger.info("="*80)

    # M√©tricas del mejor modelo
    metrics = {
        'modelo_seleccionado': best_name,
        'mape': best_results['val_metrics']['mape'],
        'rmape': best_results['val_metrics']['rmape'],
        'r2': best_results['val_metrics']['r2'],
        'mae': best_results['val_metrics']['mae'],
        'comparacion_modelos': {
            name: {
                'mape': results[1]['val_metrics']['mape'],
                'rmape': results[1]['val_metrics']['rmape'],
                'r2': results[1]['val_metrics']['r2']
            }
            for name, results in trained_models.items()
        }
    }

    return champion_path, metrics


def check_hourly_disaggregation_trained(ucp: str) -> bool:
    """
    Verifica si el sistema de desagregaci√≥n horaria est√° entrenado para un UCP espec√≠fico

    Args:
        ucp: Nombre del UCP (ej: 'Atlantico', 'Oriente')

    Returns:
        bool: True si est√° entrenado
    """
    normal_path = Path(f'models/{ucp}/hourly_disaggregator.pkl')
    special_path = Path(f'models/{ucp}/special_days_disaggregator.pkl')

    return normal_path.exists() and special_path.exists()


def train_hourly_disaggregation_if_needed(df_with_features: pd.DataFrame, ucp: str):
    """
    Entrena sistema de desagregaci√≥n horaria si no existe para un UCP espec√≠fico

    Args:
        df_with_features: DataFrame con datos hist√≥ricos y features
        ucp: Nombre del UCP (ej: 'Atlantico', 'Oriente')
    """
    if check_hourly_disaggregation_trained(ucp):
        logger.info(f"‚úì Sistema de desagregaci√≥n horaria ya est√° entrenado para {ucp}")
        return

    logger.info("="*80)
    logger.info(f"üîß ENTRENANDO SISTEMA DE DESAGREGACI√ìN HORARIA PARA {ucp}")
    logger.info("="*80)

    try:
        # Crear directorio para modelos del UCP
        models_ucp_dir = Path(f'models/{ucp}')
        models_ucp_dir.mkdir(parents=True, exist_ok=True)

        engine = HourlyDisaggregationEngine(auto_load=False)

        # Normalizar nombre de columna de fecha
        df_temp = df_with_features.copy()
        # if 'FECHA' in df_temp.columns:
        #     df_temp.rename(columns={'FECHA': 'fecha'}, inplace=True)

        # Guardar temporal para entrenamiento
        temp_path = Path(f'data/features/{ucp}/temp_for_training.csv')
        temp_path.parent.mkdir(parents=True, exist_ok=True)
        df_temp.to_csv(temp_path, index=False)

        engine.train_all(
            data_path=temp_path,
            n_clusters_normal=35,
            n_clusters_special=15,
            save=True,
            output_dir=str(models_ucp_dir)
        )

        # Eliminar temporal
        if temp_path.exists():
            temp_path.unlink()

        logger.info(f"‚úì Sistema de desagregaci√≥n horaria entrenado para {ucp}")
        logger.info("="*80)

    except Exception as e:
        logger.error(f"Error entrenando desagregaci√≥n horaria para {ucp}: {e}")
        logger.warning("Se usar√°n placeholders para distribuci√≥n horaria")


# ============================================================================
# ENDPOINTS
# ============================================================================

@app.post("/predict", response_model=PredictResponse, status_code=status.HTTP_200_OK)
async def predict_demand(request: PredictRequest):
    """
    Genera predicci√≥n de demanda energ√©tica para los pr√≥ximos N d√≠as con granularidad horaria

    Flujo:
    1. Ejecuta pipeline de feature engineering con datos hist√≥ricos hasta ayer
    2. Verifica si existe modelo entrenado (o entrena uno nuevo si se requiere)
    3. Genera predicci√≥n para los pr√≥ximos N d√≠as
    4. Desagrega cada d√≠a en 24 per√≠odos horarios (P1-P24) usando clustering K-Means
    5. Retorna array JSON con predicciones completas

    Args:
        request: PredictRequest con par√°metros de la predicci√≥n

    Returns:
        PredictResponse con array de predicciones horarias

    Raises:
        HTTPException: Si hay error en alg√∫n paso del proceso
    """
    try:
        logger.info("="*80)
        logger.info("üöÄ INICIANDO PREDICCI√ìN DE DEMANDA")
        logger.info("="*80)

        # ====================================================================
        # PASO 1: EJECUTAR PIPELINE DE FEATURE ENGINEERING
        # ====================================================================
        logger.info(f"\nüìä PASO 1: Procesando datos hist√≥ricos y creando features para {request.ucp}...")
        await run_in_threadpool(full_update_csv, request.ucp)
        try:
            # Paths din√°micos basados en UCP
            power_data_path = f'data/raw/{request.ucp}/datos.csv'
            weather_data_path = f'data/raw/{request.ucp}/clima_new.csv'
            output_dir = Path(f'data/features/{request.ucp}')

            df_with_features, _ = run_automated_pipeline(
                power_data_path=power_data_path,
                weather_data_path=weather_data_path,
                start_date='2015-01-01',
                end_date=request.end_date,
                output_dir=output_dir
            )

            logger.info(f"‚úì Pipeline completado para {request.ucp}: {len(df_with_features)} registros con {len(df_with_features.columns)} columnas")

        except FileNotFoundError as e:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Archivo no encontrado: {str(e)}"
            )
        except Exception as e:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Error en pipeline de datos: {str(e)}"
            )

        # ====================================================================
        # PASO 2: VERIFICAR/ENTRENAR MODELO
        # ====================================================================
        logger.info(f"\nü§ñ PASO 2: Verificando modelo de predicci√≥n para {request.ucp}...")

        try:
            model_path, train_metrics = train_model_if_needed(
                df_with_features=df_with_features,
                ucp=request.ucp,
                force_retrain=request.force_retrain
            )

            modelo_entrenado = len(train_metrics) > 0

            if modelo_entrenado:
                logger.info(f"‚úì Modelo entrenado exitosamente")
                logger.info(f"  MAPE: {train_metrics['mape']:.4f}%")
                logger.info(f"  rMAPE: {train_metrics['rmape']:.4f}")
                logger.info(f"  R¬≤: {train_metrics['r2']:.4f}")
            else:
                logger.info(f"‚úì Usando modelo existente: {model_path.name}")

        except Exception as e:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Error en entrenamiento de modelo: {str(e)}"
            )

        # ====================================================================
        # PASO 3: VERIFICAR/ENTRENAR SISTEMA DE DESAGREGACI√ìN HORARIA
        # ====================================================================
        logger.info(f"\n‚è∞ PASO 3: Verificando sistema de desagregaci√≥n horaria para {request.ucp}...")

        try:
            logger.info(f"   Verificando si desagregaci√≥n horaria est√° entrenada...")
            train_hourly_disaggregation_if_needed(df_with_features, request.ucp)
            logger.info(f"Desagrecacion horaria se ejecuta")

        except Exception as e:
            logger.warning(f"‚ö† Error en desagregaci√≥n horaria: {e}")
            logger.warning("Se continuar√° con placeholders")

        # ====================================================================
        # PASO 4: GENERAR PREDICCIONES
        # ====================================================================
        logger.info(f"\nüîÆ PASO 4: Generando predicciones para {request.n_days} d√≠as...")

        try:












            climate_raw_path = f'data/raw/{request.ucp}/clima_new.csv'






            df_try_features = df_with_features.copy()
            max_date = df_with_features['FECHA'].max()
            cut_date = max_date - pd.Timedelta(days=30)

            df_try_features = df_try_features[df_try_features['FECHA'] <= cut_date]        
            temp_try_path = f'data/features/{request.ucp}/temp_api_features_try.csv'
            df_try_features.to_csv(temp_try_path, index=False)
            # Inicializar pipeline de predicci√≥n con datos RECIEN PROCESADOS
            pipeline = ForecastPipeline(
                model_path=str(model_path),
                historical_data_path=temp_try_path,
                festivos_path='config/festivos.json',
                enable_hourly_disaggregation=True,  # ‚Üê Habilitado con nuevo modelo
                raw_climate_path=climate_raw_path,
                ucp=request.ucp  # ‚Üê Pasar UCP al pipeline
            )
            check_date=max_date - pd.Timedelta(days=29)
            # Generar predicciones
            predictions_df = pipeline.predict_next_n_days(n_days=30)
            print('predictions_df'*40)
            print(predictions_df)
            
            mape_check_df=df_with_features[df_with_features['FECHA'] >= check_date] 
            print(mape_check_df)  
            
            import numpy as np
            

            # --- Alinear por fecha ---
            pred = predictions_df.copy()
            real = mape_check_df.copy()

            pred['fecha'] = pd.to_datetime(pred['fecha'])
            real['FECHA'] = pd.to_datetime(real['FECHA'])

            # Unimos por fecha
            df_merged = pred.merge(real, left_on='fecha', right_on='FECHA', how='inner')
            print(df_merged.columns)
            # ============================
            # 1Ô∏è‚É£ MAPE TOTAL (demanda_predicha vs TOTAL)
            # ============================
            df_merged['abs_pct_error'] = np.abs(
                (df_merged['demanda_predicha'] - df_merged['TOTAL']) *100/ df_merged['TOTAL']
            )
            # cols_xy = ["FECHA"]+[f'P{i}_x' for i in range(1, 25)] + [f'P{i}_y' for i in range(1, 25)]
        

            # def calcular_mape_por_dia(df):
            #     resultados = []

            #     for idx, row in df.iterrows():
            #         fecha = row["FECHA"]

            #         # Extraer columnas reales y predichas
            #         reales = [row[f"P{h}_x"] for h in range(1,25)]
            #         preds  = [row[f"P{h}_y"] for h in range(1,25)]

            #         # Calcular MAPE hora por hora
            #         errores = []
            #         for r, p in zip(reales, preds):
            #             if r == 0:
            #                 errores.append(0)
            #             else:
            #                 errores.append(abs((r - p) / r))

            #         mape_dia = np.mean(errores) * 100

            #         resultados.append({
            #             "FECHA": fecha,
            #             "MAPE": mape_dia
            #         })

            #     return pd.DataFrame(resultados)

            # df_mape = calcular_mape_por_dia(df_merged[cols_xy])
            #print(df_mape)
            print(df_merged[['abs_pct_error','demanda_predicha','TOTAL']])
            # Condici√≥n: error mayor al 5%
            cond = df_merged['abs_pct_error'] > 5
            print(cond)
            # Detectar si hay dos True consecutivos
            hay_dos_seguidos = (cond & cond.shift(1)).any()

            print("¬øHay dos errores seguidos > 5%?:", hay_dos_seguidos)
            
            mape_total = df_merged['abs_pct_error'].mean()
            print("MAPE TOTAL:", mape_total)
            print("MAPE TOTAL:", df_merged[['abs_pct_error','demanda_predicha','TOTAL']])

            print('df_try_features'*40)

            # ====================================================================
            # AN√ÅLISIS DE CAUSALIDAD CON OPENAI (si se requiere reentrenamiento)
            # ====================================================================

            # Determinar si se requiere reentrenamiento y tipo de error
            if mape_total > 5 and hay_dos_seguidos:
                should_retrain = True
                error_type = 'ambos'
                reason_base = f'Error mensual superior al 5% (MAPE: {mape_total:.2f}%) y dos d√≠as consecutivos con error superior al 5%'
                logger.info(f"‚ö† MAPE TOTAL: {mape_total:.2f}%. Se requiere reentrenamiento.")
            elif hay_dos_seguidos:
                should_retrain = True
                error_type = 'consecutivo'
                reason_base = f'Dos d√≠as consecutivos con error superior al 5% (MAPE mensual: {mape_total:.2f}%)'
                logger.info(f"‚ö† MAPE TOTAL: {mape_total:.2f}%. Se requiere reentrenamiento.")
            elif mape_total > 5:
                should_retrain = True
                error_type = 'mensual'
                reason_base = f'Error mensual superior al 5% (MAPE: {mape_total:.2f}%)'
                logger.info(f"‚ö† MAPE TOTAL: {mape_total:.2f}%. Se requiere reentrenamiento.")
            else:
                should_retrain = False
                error_type = None
                reason = f'Error dentro de l√≠mites aceptables (MAPE: {mape_total:.2f}%)'
                logger.info(f"‚úì MAPE TOTAL: {mape_total:.2f}%. No se requiere reentrenamiento.")

            # Si se requiere reentrenamiento, analizar causas con OpenAI
            if should_retrain:
                logger.info("="*80)
                logger.info("üîç ANALIZANDO CAUSAS DEL ERROR CON OPENAI")
                logger.info("="*80)

                # Extraer fechas de d√≠as con errores consecutivos si aplica
                dias_consecutivos = None
                if error_type in ['consecutivo', 'ambos']:
                    # Encontrar los d√≠as consecutivos con error > 5%
                    mask_consecutivos = cond & cond.shift(1)
                    indices_consecutivos = df_merged.index[mask_consecutivos].tolist()

                    # Convertir a datetime si no lo est√° y formatear
                    if len(indices_consecutivos) > 0:
                        fechas_consecutivas = []
                        for idx in indices_consecutivos:
                            fecha_val = df_merged.loc[idx, 'FECHA']
                            if isinstance(fecha_val, pd.Timestamp):
                                fechas_consecutivas.append(fecha_val.strftime('%Y-%m-%d'))
                            else:
                                fechas_consecutivas.append(str(fecha_val))

                        # Tambi√©n incluir el d√≠a anterior al primero marcado
                        if fechas_consecutivas and indices_consecutivos:
                            primer_idx = indices_consecutivos[0]
                            if primer_idx > 0:
                                fecha_ant_val = df_merged.loc[primer_idx - 1, 'FECHA']
                                if isinstance(fecha_ant_val, pd.Timestamp):
                                    fecha_anterior = fecha_ant_val.strftime('%Y-%m-%d')
                                else:
                                    fecha_anterior = str(fecha_ant_val)
                                dias_consecutivos = [fecha_anterior] + fechas_consecutivas
                            else:
                                dias_consecutivos = fechas_consecutivas

                    logger.info(f"  D√≠as consecutivos identificados: {dias_consecutivos}")

                # Obtener rango de fechas del an√°lisis
                fecha_min = df_merged['FECHA'].min()
                fecha_max = df_merged['FECHA'].max()

                if isinstance(fecha_min, pd.Timestamp):
                    fecha_inicio_analisis = fecha_min.strftime('%Y-%m-%d')
                else:
                    fecha_inicio_analisis = str(fecha_min)

                if isinstance(fecha_max, pd.Timestamp):
                    fecha_fin_analisis = fecha_max.strftime('%Y-%m-%d')
                else:
                    fecha_fin_analisis = str(fecha_max)

                # Llamar a OpenAI para an√°lisis de causalidad
                openai_analysis = await analyze_error_with_openai(
                    ucp=request.ucp,
                    error_type=error_type,
                    mape_total=mape_total,
                    fecha_inicio=fecha_inicio_analisis,
                    fecha_fin=fecha_fin_analisis,
                    dias_consecutivos=dias_consecutivos
                )

                # Combinar reason base con an√°lisis de OpenAI
                reason = f"{reason_base}\n\nüìä An√°lisis de causalidad:\n{openai_analysis}"

                logger.info(f"‚úì An√°lisis de causalidad agregado al reporte")
                logger.info("="*80)





































            # Determinar ruta de datos clim√°ticos RAW espec√≠ficos del UCP
            climate_raw_path = f'data/raw/{request.ucp}/clima_new.csv'

            # CRITICO: Guardar datos procesados temporalmente en directorio del UCP
            temp_features_path = f'data/features/{request.ucp}/temp_api_features.csv'
            df_with_features.to_csv(temp_features_path, index=False)
            
            # Log datos guardados (detectar columna de fecha)
            if 'FECHA' in df_with_features.columns:
                logger.info(f"   √öltima fecha en temp: {df_with_features['FECHA'].max()}")
            elif 'fecha' in df_with_features.columns:
                print('fecha'*40)
                logger.info(f"   √öltima fecha en temp: {df_with_features['fecha'].max()}")
            else:
                logger.info(f"   Datos guardados en temp (sin columna fecha expl√≠cita)")
            logger.info(f"   Total filas: {len(df_with_features)}")
            
            # Inicializar pipeline de predicci√≥n con datos RECIEN PROCESADOS
            pipeline = ForecastPipeline(
                model_path=str(model_path),
                historical_data_path=temp_features_path,
                festivos_path='config/festivos.json',
                enable_hourly_disaggregation=True,  # ‚Üê Habilitado con nuevo modelo
                raw_climate_path=climate_raw_path,
                ucp=request.ucp  # ‚Üê Pasar UCP al pipeline
            )

            # Generar predicciones
            predictions_df = pipeline.predict_next_n_days(n_days=request.n_days)
            
            # Limpiar archivo temporal
            import os
            if os.path.exists(temp_features_path):
                os.remove(temp_features_path)

            logger.info(f"‚úì Predicciones generadas: {len(predictions_df)} d√≠as")

        except Exception as e:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Error generando predicciones: {str(e)}\n{traceback.format_exc()}"
            )

        # ====================================================================
        # PASO 5: FORMATEAR RESPUESTA
        # ====================================================================
        logger.info("\nüìã PASO 5: Formateando respuesta...")

        try:
            # Convertir DataFrame a lista de diccionarios
            predictions_list = []

            # Mapeo de d√≠as de la semana
            dias_semana = ['Lunes', 'Martes', 'Mi√©rcoles', 'Jueves', 'Viernes', 'S√°bado', 'Domingo']

            for _, row in predictions_df.iterrows():
                fecha = pd.to_datetime(row['fecha'])

                # Determinar m√©todo de desagregaci√≥n usado
                metodo = row.get('metodo_desagregacion', 'special' if row.get('is_festivo', False) else 'normal')

                prediction = {
                    'fecha': fecha.strftime('%Y-%m-%d'),
                    'dia_semana': dias_semana[row.get('dayofweek', fecha.dayofweek)],
                    'demanda_total': round(float(row['demanda_predicha']), 2),
                    'is_festivo': bool(row.get('is_festivo', False)),
                    'is_weekend': bool(row.get('is_weekend', False)),
                    'metodo_desagregacion': metodo,
                    'cluster_id': int(row['cluster_id']) if pd.notna(row.get('cluster_id')) else None,
                    **{f'P{i}': round(float(row.get(f'P{i}', 0)), 2) for i in range(1, 25)},
                    **{f'senda_P{i}': round(float(row.get(f'senda_P{i}', 0)), 6) if pd.notna(row.get(f'senda_P{i}')) else None for i in range(1, 25)}
                }

                predictions_list.append(prediction)

            # Calcular estad√≠sticas
            metadata = {
                'fecha_generacion': datetime.now().isoformat(),
                'modelo_usado': model_path.stem,
                'dias_predichos': len(predictions_df),
                'fecha_inicio': predictions_df['fecha'].min().strftime('%Y-%m-%d'),
                'fecha_fin': predictions_df['fecha'].max().strftime('%Y-%m-%d'),
                'demanda_promedio': round(float(predictions_df['demanda_predicha'].mean()), 2),
                'demanda_min': round(float(predictions_df['demanda_predicha'].min()), 2),
                'demanda_max': round(float(predictions_df['demanda_predicha'].max()), 2),
                'dias_laborables': int((predictions_df['is_weekend'] == False).sum()),
                'dias_fin_de_semana': int((predictions_df['is_weekend'] == True).sum()),
                'dias_festivos': int((predictions_df['is_festivo'] == True).sum()),
                'modelo_entrenado': modelo_entrenado,
                'metricas_modelo': train_metrics if modelo_entrenado else {}
            }

            logger.info("‚úì Respuesta formateada correctamente")
            logger.info(f"  Demanda promedio: {metadata['demanda_promedio']:,.2f} MWh")
            logger.info(f"  Rango: {metadata['demanda_min']:,.2f} - {metadata['demanda_max']:,.2f} MWh")

        except Exception as e:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Error formateando respuesta: {str(e)}"
            )

        # ====================================================================
        # RESPUESTA FINAL
        # ====================================================================
        logger.info("\n" + "="*80)
        logger.info("‚úÖ PREDICCI√ìN COMPLETADA EXITOSAMENTE")
        logger.info("="*80 + "\n")

        return PredictResponse(
            should_retrain=should_retrain,
            reason=reason,
            status="success",
            message=f"Predicci√≥n generada exitosamente para {request.n_days} d√≠as con granularidad horaria",
            metadata=metadata,
            predictions=predictions_list
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error inesperado: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error inesperado en el servidor: {str(e)}"
        )


@app.get("/health", response_model=HealthResponse, status_code=status.HTTP_200_OK)
async def health_check(ucp: Optional[str] = None):
    """
    Health check del sistema

    Verifica estado de:
    - Modelo de predicci√≥n
    - Sistema de desagregaci√≥n horaria
    - Datos hist√≥ricos

    Args:
        ucp: (Opcional) Nombre del UCP para verificar estado espec√≠fico

    Returns:
        HealthResponse con estado de componentes
    """
    components = {}

    if ucp:
        # Verificar modelo espec√≠fico del UCP
        model_exists, model_path = check_model_exists(ucp)
        components['prediction_model'] = {
            'status': 'healthy' if model_exists else 'missing',
            'ucp': ucp,
            'path': str(model_path) if model_path else None,
            'last_modified': model_path.stat().st_mtime if model_path and model_path.exists() else None
        }

        # Verificar desagregaci√≥n horaria del UCP
        hourly_trained = check_hourly_disaggregation_trained(ucp)
        components['hourly_disaggregation'] = {
            'status': 'healthy' if hourly_trained else 'missing',
            'ucp': ucp,
            'models': {
                'normal': Path(f'models/{ucp}/hourly_disaggregator.pkl').exists(),
                'special': Path(f'models/{ucp}/special_days_disaggregator.pkl').exists()
            }
        }

        # Verificar datos hist√≥ricos del UCP
        features_path = Path(f'data/features/{ucp}/data_with_features_latest.csv')
        components['historical_data'] = {
            'status': 'healthy' if features_path.exists() else 'missing',
            'ucp': ucp,
            'path': str(features_path) if features_path.exists() else None,
            'last_modified': features_path.stat().st_mtime if features_path.exists() else None
        }
    else:
        # Verificar sistema general (retrocompatibilidad)
        # Buscar todos los UCPs disponibles
        models_base = Path('models')
        ucps_disponibles = [d.name for d in models_base.iterdir() if d.is_dir() and (d / 'registry').exists()]

        components['system'] = {
            'status': 'healthy',
            'ucps_disponibles': ucps_disponibles,
            'total_ucps': len(ucps_disponibles)
        }

    # Estado general
    all_healthy = all(comp.get('status') == 'healthy' for comp in components.values())

    return HealthResponse(
        status='healthy' if all_healthy else 'degraded',
        timestamp=datetime.now().isoformat(),
        version='1.0.0',
        components=components
    )


@app.get("/models", status_code=status.HTTP_200_OK)
async def list_models(ucp: Optional[str] = None):
    """
    Lista modelos disponibles en el sistema

    Args:
        ucp: (Opcional) Nombre del UCP para listar modelos espec√≠ficos

    Returns:
        Dict con informaci√≥n de modelos entrenados
    """
    if ucp:
        # Listar modelos de un UCP espec√≠fico
        models_dir = Path(f'models/{ucp}/trained')
        registry_path = Path(f'models/{ucp}/registry/champion_model.joblib')

        models = []

        # Listar modelos entrenados del UCP
        if models_dir.exists():
            for model_file in sorted(models_dir.glob('*.joblib'), key=lambda p: p.stat().st_mtime, reverse=True):
                models.append({
                    'name': model_file.stem,
                    'path': str(model_file),
                    'size_mb': round(model_file.stat().st_size / (1024 * 1024), 2),
                    'created': datetime.fromtimestamp(model_file.stat().st_mtime).isoformat()
                })

        # Modelo campe√≥n del UCP
        champion = None
        if registry_path.exists():
            champion = {
                'name': 'champion_model',
                'path': str(registry_path),
                'size_mb': round(registry_path.stat().st_size / (1024 * 1024), 2),
                'created': datetime.fromtimestamp(registry_path.stat().st_mtime).isoformat()
            }

        return {
            'ucp': ucp,
            'total_models': len(models),
            'champion': champion,
            'models': models
        }
    else:
        # Listar todos los UCPs y sus modelos
        models_base = Path('models')
        ucps_info = []

        if models_base.exists():
            for ucp_dir in models_base.iterdir():
                if ucp_dir.is_dir():
                    registry_path = ucp_dir / 'registry' / 'champion_model.joblib'
                    trained_dir = ucp_dir / 'trained'

                    if registry_path.exists() or (trained_dir.exists() and list(trained_dir.glob('*.joblib'))):
                        ucps_info.append({
                            'ucp': ucp_dir.name,
                            'has_champion': registry_path.exists(),
                            'trained_models': len(list(trained_dir.glob('*.joblib'))) if trained_dir.exists() else 0,
                            'champion_path': str(registry_path) if registry_path.exists() else None
                        })

        return {
            'total_ucps': len(ucps_info),
            'ucps': ucps_info,
            'note': 'Use ?ucp=<name> to get detailed info for a specific UCP'
        }


@app.get("/", status_code=status.HTTP_200_OK)
async def root():
    """
    Endpoint ra√≠z con informaci√≥n de la API
    """
    return {
        "api": "EPM Energy Demand Forecasting API",
        "version": "1.0.0",
        "status": "operational",
        "docs": "/docs",
        "endpoints": {
            "POST /predict": "Genera predicci√≥n de demanda con granularidad horaria",
            "GET /health": "Estado del sistema",
            "GET /models": "Lista de modelos disponibles"
        }
    }


# ============================================================================
# INICIALIZACI√ìN
# ============================================================================

@app.on_event("startup")
async def startup_event():
    """Evento de inicializaci√≥n de la API"""
    logger.info("="*80)
    logger.info("üöÄ INICIANDO API DE PRON√ìSTICO DE DEMANDA ENERG√âTICA - EPM")
    logger.info("="*80)
    logger.info(f"Versi√≥n: 1.0.0")
    logger.info(f"Documentaci√≥n: http://localhost:8000/docs")
    logger.info("="*80)


@app.on_event("shutdown")
async def shutdown_event():
    """Evento de cierre de la API"""
    logger.info("üõë Apagando API...")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
